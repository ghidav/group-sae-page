<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta name="keywords"
    content="sparse autoencoders, SAE, interpretability, large language models, LLM, efficiency, layer clustering, Group-SAE">
  <meta name="author" content="Davide Ghilardi, Federico Belotti, Marco Molinari, Tao Ma, Matteo Palmonari">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Group-SAE">
  <meta property="og:title"
    content="Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta property="og:description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta property="og:url" content="https://github.com/ghidav/group-sae-page">
  <meta property="og:image" content="static/images/main.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Group-SAE Research Team">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="sparse autoencoders">
  <meta property="article:tag" content="interpretability">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title"
    content="Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="twitter:description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta name="twitter:image" content="static/images/main.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title"
    content="Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="citation_author" content="Davide Ghilardi, Federico Belotti, Marco Molinari, Tao Ma, Matteo Palmonari">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="Research Paper">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups",
    "description": "Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.",
    "author": [
      {
        "@type": "Person",
        "name": "Davide Ghilardi",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      },
      {
        "@type": "Person",
        "name": "Federico Belotti",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      },
      {
        "@type": "Person",
        "name": "Marco Molinari",
        "affiliation": {
          "@type": "Organization",
          "name": "London School of Economics"
        }
      },
      {
        "@type": "Person",
        "name": "Tao Ma",
        "affiliation": {
          "@type": "Organization",
          "name": "London School of Economics"
        }
      },
      {
        "@type": "Person",
        "name": "Matteo Palmonari",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "Research Publication"
    },
    "url": "https://github.com/ghidav/group-sae-page",
    "image": "static/images/main.png",
    "keywords": ["sparse autoencoders", "SAE", "interpretability", "large language models", "efficiency"],
    "abstract": "Sparse Autoencoders (SAEs) have recently been employed as a promising unsupervised approach for understanding the representations of layers of Large Language Models (LLMs). However, with the growth in model size and complexity, training SAEs is computationally intensive. We propose Group-SAE, a novel strategy that groups similar layers and trains a single SAE per group, significantly reducing training costs.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://github.com/ghidav/group-sae-page"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Machine Learning Interpretability"
      },
      {
        "@type": "Thing",
        "name": "Sparse Autoencoders"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Group-SAE Research",
    "url": "https://github.com/your-username/group-sae-page",
    "logo": "static/images/favicon.ico"
  }
  </script>
</head>

<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!--
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <div class="work-item" style="text-align: center; padding: 20px; color: #666;">
          <p>Related works will be added here soon.</p>
        </div>
      </div>
    </div>
  </div>
  -->

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Efficient Training of Sparse Autoencoders for Large Language
                Models via Layer Groups</h1>
              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your paper authors and their personal links -->
                <span class="author-block">
                  <a href="https://ghidav.github.io/" target="_blank">Davide Ghilardi</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://github.com/belerico" target="_blank">Federico Belotti</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.marcomolinari.io/" target="_blank">Marco Molinari</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://xiaobaobaochifan.github.io/" target="_blank">Tao Ma</a>,
                </span>
                <span class="author-block">
                  <a href="https://www.unimib.it/matteo-luigi-palmonari" target="_blank">Matteo Palmonari</a>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your institution and conference/journal info -->
                <span class="author-block">University of Milano Bicocca, LSE<br>EMNLP 2025</span>
                <!-- TODO: Remove this line if no equal contribution -->
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <p class="image">
          <video poster="" id="LaDeDa" autoplay controls muted loop height="100%">
            <source src="static/videos/main.mov" type="video/mp4">
          </video>
        </p>
      </div>
    </section>

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Sparse Autoencoders (SAEs) have recently been employed as a promising unsupervised approach for
                understanding the representations of layers of Large Language Models (LLMs). However, with the growth in
                model size and complexity, training SAEs is computationally intensive, as typically one SAE is trained
                for each model layer. To address such limitation, we propose <strong>Group-SAE</strong>, a novel
                strategy to train SAEs. Our method considers the similarity of the residual stream representations
                between contiguous layers to group similar layers and train a single SAE per group. To balance the
                trade-off between efficiency and performance, we further introduce <strong>AMAD</strong> (Average
                Maximum Angular Distance), an empirical metric that guides the selection of an optimal number of groups
                based on representational similarity across layers. Experiments on models from the Pythia family show
                that our approach significantly accelerates training with minimal impact on reconstruction quality and
                comparable downstream task performance and interpretability over baseline SAEs trained layer by layer.
                This method provides an efficient and scalable strategy for training SAEs in modern LLMs.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->


    <!-- Method Section -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Group-SAE</h2>
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <div class="content">
                <p>Our method leverages the observation that nearby neural network layers tend to learn similar
                  representations. Instead of training individual SAEs for each layer, we:</p>
                <ol>
                  <li><strong>Compute Angular Distances:</strong> Calculate mean angular distances between residual
                    stream activations of different layers</li>
                  <li><strong>Hierarchical Clustering:</strong> Use agglomerative clustering with complete linkage to
                    group similar layers</li>
                  <li><strong>Optimal Group Selection:</strong> Apply AMAD (Average Maximum Angular Distance) metric to
                    determine the optimal number of groups</li>
                  <li><strong>Efficient Training:</strong> Train a single SAE per group, significantly reducing
                    computational costs</li>
                </ol>

                <h3>AMAD Metric</h3>
                <p>The Average Maximum Angular Distance quantifies the average worst-case distance within each group:
                </p>
                <p style="text-align: center; font-style: italic;">AMAD(G) = (1/G) × Σ D_g</p>
                <p>We identify an optimal threshold around AMAD ≈ 0.2, beyond which reconstruction performance degrades
                  more rapidly.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End method section -->


    <!-- Evaluations -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Evaluations</h2>
          <div class="columns">
            <h3 class="title is-4">Reconstruction vs. Similarity</h3>
            <div class="column is-half">
              <img src="static/images/eval/amd_vs_fvu.png"
                alt="Feature concordance heatmap for Pythia-160M" loading="lazy" style="width: 100%;" />
              <p class="has-text-centered"><em>Mean Maximum Concordance (MMCO) showing feature similarity between
                  baseline and Group-SAEs.</em></p>
            </div>
            <div class="column is-half">
              <img src="static/images/eval/amd_vs_delta_ce.png" alt="Feature distribution across layers"
                loading="lazy" style="width: 100%;" />
              <p class="has-text-centered"><em>Feature activation distribution across layers within groups, showing how
                  features spread across similar layers.</em></p>
            </div>
          </div>
          <div class="columns">
            <h3 class="title is-4">Reconstruction vs. Compute</h3>
            <div class="column is-half">
              <img src="static/images/eval/relative_flops_vs_fvu.png"
                alt="Feature concordance heatmap for Pythia-160M" loading="lazy" style="width: 100%;" />
              <p class="has-text-centered"><em>Mean Maximum Concordance (MMCO) showing feature similarity between
                  baseline and Group-SAEs.</em></p>
            </div>
            <div class="column is-half">
              <img src="static/images/eval/relative_flops_vs_delta_ce.png" alt="Feature distribution across layers"
                loading="lazy" style="width: 100%;" />
              <p class="has-text-centered"><em>Feature activation distribution across layers within groups, showing how
                  features spread across similar layers.</em></p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End additional analysis -->


    <!-- Additional Analysis -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Feature Analysis</h2>
          <div class="columns">
            <div class="column is-half">
              <h3 class="title is-4">Autointerpretability</h3>
              <img src="static/images/concordance/concordance_pythia-160m_all.png"
                alt="Feature concordance heatmap for Pythia-160M" loading="lazy" style="width: 100%;" />
              <p class="has-text-centered"><em>Mean Maximum Concordance (MMCO) showing feature similarity between
                  baseline and Group-SAEs.</em></p>
            </div>
            <div class="column is-half">
              <h3 class="title is-4">Feature Concordance</h3>
              <img src="static/images/concordance/concordance_pythia-160m_all.png"
                alt="Feature concordance heatmap for Pythia-160M" loading="lazy" style="width: 100%;" />
              <p class="has-text-centered"><em>Mean Maximum Concordance (MMCO) showing feature similarity between
                  baseline and Group-SAEs.</em></p>
            </div>
            <div class="column is-half">
              <h3 class="title is-4">Feature Distribution</h3>
              <img src="static/images/spreading/feat_spread_pythia-160m.png" alt="Feature distribution across layers"
                loading="lazy" style="width: 100%;" />
              <p class="has-text-centered"><em>Feature activation distribution across layers within groups, showing how
                  features spread across similar layers.</em></p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End additional analysis -->


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{GroupSAE2024,
  title={Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups},
  author={Group-SAE Research Team},
  year={2024},
  url={https://github.com/ghidav/group-sae}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>