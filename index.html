<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title"
    content="Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta name="keywords"
    content="sparse autoencoders, SAE, interpretability, large language models, LLM, efficiency, layer clustering, Group-SAE">
  <meta name="author" content="Davide Ghilardi, Federico Belotti, Marco Molinari, Tao Ma, Matteo Palmonari">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Group-SAE">
  <meta property="og:title"
    content="Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta property="og:description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta property="og:url" content="https://github.com/ghidav/group-sae-page">
  <meta property="og:image" content="static/images/main.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Group-SAE Research Team">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="sparse autoencoders">
  <meta property="article:tag" content="interpretability">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title"
    content="Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="twitter:description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta name="twitter:image" content="static/images/main.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title"
    content="Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="citation_author" content="Davide Ghilardi, Federico Belotti, Marco Molinari, Tao Ma, Matteo Palmonari">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="Research Paper">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups",
    "description": "Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.",
    "author": [
      {
        "@type": "Person",
        "name": "Davide Ghilardi",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      },
      {
        "@type": "Person",
        "name": "Federico Belotti",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      },
      {
        "@type": "Person",
        "name": "Marco Molinari",
        "affiliation": {
          "@type": "Organization",
          "name": "London School of Economics"
        }
      },
      {
        "@type": "Person",
        "name": "Tao Ma",
        "affiliation": {
          "@type": "Organization",
          "name": "London School of Economics"
        }
      },
      {
        "@type": "Person",
        "name": "Matteo Palmonari",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "Research Publication"
    },
    "url": "https://github.com/ghidav/group-sae-page",
    "image": "static/images/main.png",
    "keywords": ["sparse autoencoders", "SAE", "interpretability", "large language models", "efficiency"],
    "abstract": "Sparse Autoencoders (SAEs) have recently been employed as a promising unsupervised approach for understanding the representations of layers of Large Language Models (LLMs). However, with the growth in model size and complexity, training SAEs is computationally intensive. We propose Group-SAE, a novel strategy that groups similar layers and trains a single SAE per group, significantly reducing training costs.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://github.com/ghidav/group-sae-page"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Machine Learning Interpretability"
      },
      {
        "@type": "Thing",
        "name": "Sparse Autoencoders"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Group-SAE Research",
    "url": "https://github.com/your-username/group-sae-page",
    "logo": "static/images/favicon.ico"
  }
  </script>

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      }
    };
  </script>
  <script defer id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!--
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <div class="work-item" style="text-align: center; padding: 20px; color: #666;">
          <p>Related works will be added here soon.</p>
        </div>
      </div>
    </div>
  </div>
  -->

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Group-SAE: Efficient Training of Sparse Autoencoders for Large
                Language Models via Layer Groups</h1>
              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your paper authors and their personal links -->
                <span class="author-block">
                  <a href="https://ghidav.github.io/" target="_blank">Davide Ghilardi</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://github.com/belerico" target="_blank">Federico Belotti</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.marcomolinari.io/" target="_blank">Marco Molinari</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://xiaobaobaochifan.github.io/" target="_blank">Tao Ma</a>,
                </span>
                <span class="author-block">
                  <a href="https://www.unimib.it/matteo-luigi-palmonari" target="_blank">Matteo Palmonari</a>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your institution and conference/journal info -->
                <span class="author-block">University of Milano Bicocca, LSE<br>EMNLP 2025</span>
                <!-- TODO: Remove this line if no equal contribution -->
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2410.21508" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/ghidav/group-sae" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2410.21508" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/Group-SAE" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="static/images/hf-logo.svg" alt="HF Logo" width="24" height="24">
                      </span>
                      <span>Models</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      </div>
    </section>

    <section class="hero teaser" style="margin-bottom: 3rem;">
      <div class="container is-max-desktop">
        <p class="image">
          <video poster="" id="main" autoplay controls muted loop height="100%">
            <source src="static/videos/main.mp4" type="video/mp4">
          </video>
        </p>
      </div>
    </section>

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Sparse Autoencoders (SAEs) have recently been employed as a promising unsupervised approach for
                understanding the representations of layers of Large Language Models (LLMs). However, with the growth in
                model size and complexity, training SAEs is computationally intensive, as typically one SAE is trained
                for each model layer. To address such limitation, we propose <strong>Group-SAE</strong>, a novel
                strategy to train SAEs. Our method considers the similarity of the residual stream representations
                between contiguous layers to group similar layers and train a single SAE per group. To balance the
                trade-off between efficiency and performance, we further introduce <strong>AMAD</strong> (Average
                Maximum Angular Distance), an empirical metric that guides the selection of an optimal number of groups
                based on representational similarity across layers. Experiments on models from the Pythia family show
                that our approach significantly accelerates training with minimal impact on reconstruction quality and
                comparable downstream task performance and interpretability over baseline SAEs trained layer by layer.
                This method provides an efficient and scalable strategy for training SAEs in modern LLMs.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->


    <!-- Method Section -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h2 class="title">Group-SAE</h2>
              <div class="content">
                <p>
                  Group-SAE is a novel technique that reduces the computational demand for training a suite of SAEs for
                  a given model.
                  We group model layers by the cosine similarity of their activations and train a single SAE per group.
                </p>

                <h3>Grouping strategy</h3>
                <p>
                  Our grouping strategy consists of four steps:
                <ol>
                  <li>We cache 10M activations for each layer of the model from tokens of its training dataset. </li>
                  <li>We compute the average cosine distance for each pair of layer. From this process, we obtain a
                    distance matrix.</li>
                  <li>We apply <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" target="_blank"
                      rel="noopener noreferrer">hierarchical clustering</a> on the distance matrix to obtain the
                    partitions in $g=1,…,G$ groups.</li>
                  <li>The last step consists of computing the AMAD, the average maximum angular distance of groups, for
                    each partition.</li>
                </ol>
                <p>
                  Given the AMAD computed at step 4., one can choose the optimal number of groups $\widehat{G}$ as $\widehat{G} = \min \{G \in \{1, ..., L-1\} \mid \text{AMAD}(G) < \theta \}$.
                  We identify an optimal threshold around $\theta \approx 0.2$, beyond which reconstruction performance degrades more rapidly.
                  Intuitively, when $G$ is small, each group aggregates more distant layers, which increases AMAD; 
                  conversely, when $G=L$ (one group per layer), AMAD becomes zero but no computational savings are achieved. 
                  The goal is therefore to select the smallest $G$ such that the groups remain sufficiently homogeneous, i.e., 
                  AMAD$(G)$ stays below a target threshold.
                </p>

                <div class="container is-max-desktop">
                  <p class="image">
                    <img src="static/gifs/grouping.gif" alt="Description of GIF" style="width: 100%; height: auto;">
                  </p>
                </div>

                <h3>SAE training</h3>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End method section -->


    <!-- Evaluations -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h2 class="title">Evaluations</h2>

              <h3 class="title is-4">Comparison with baselines</h3>
              <div class="content">
                <p class="has-text-justified">
                  FVU and $\Delta\text{CE}$ for different approaches across model sizes. Our AMAD-based grouping
                  strategy
                  attains lower FVU and $\Delta\text{CE}$ than the baselines: evenly spaced groups and smaller SAEs
                  trained on all layers. Both baselines use the same training FLOPs as our method. Percentages denote
                  improvement over the <em>Smaller SAEs (All layers)</em> baseline (positive&nbsp;=&nbsp;better).
                </p>
              </div>
              <div class="table-container">
                <table class="table is-striped is-hoverable is-fullwidth evaluation-table">
                  <thead>
                    <tr>
                      <th rowspan="2">Approach</th>
                      <th colspan="2" class="has-text-centered">Pythia-160M</th>
                      <th colspan="2" class="has-text-centered">Pythia-410M</th>
                      <th colspan="2" class="has-text-centered">Pythia-1B</th>
                    </tr>
                    <tr>
                      <th>FVU</th>
                      <th>$\Delta\text{CE}_{\%}$</th>
                      <th>FVU</th>
                      <th>$\Delta\text{CE}_{\%}$</th>
                      <th>FVU</th>
                      <th>$\Delta\text{CE}_{\%}$</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <th scope="row">Group SAEs (AMAD with $\widehat{G}$ groups)</th>
                      <td><strong>0.108</strong> <span class="is-size-7 has-text-grey">(+6.1%)</span></td>
                      <td>6.01 <span class="is-size-7 has-text-grey">(+18.5%)</span></td>
                      <td><strong>0.138</strong> <span class="is-size-7 has-text-grey">(+5.5%)</span></td>
                      <td><strong>5.94</strong> <span class="is-size-7 has-text-grey">(+16.3%)</span></td>
                      <td><strong>0.182</strong> <span class="is-size-7 has-text-grey">(+3.2%)</span></td>
                      <td><strong>6.43</strong> <span class="is-size-7 has-text-grey">(+20.6%)</span></td>
                    </tr>
                    <tr>
                      <th scope="row">Group SAEs (Evenly spaced with $\widehat{G}$ groups)</th>
                      <td>0.114 <span class="is-size-7 has-text-grey">(+0.9%)</span></td>
                      <td><strong>5.40</strong> <span class="is-size-7 has-text-grey">(+26.7%)</span></td>
                      <td>0.145 <span class="is-size-7 has-text-grey">(+0.7%)</span></td>
                      <td>6.01 <span class="is-size-7 has-text-grey">(+15.4%)</span></td>
                      <td>0.189 <span class="is-size-7 has-text-grey">(-0.5%)</span></td>
                      <td>6.63 <span class="is-size-7 has-text-grey">(+18.1%)</span></td>
                    </tr>
                    <tr>
                      <th scope="row">Smaller SAEs (All layers, same PFLOPs)</th>
                      <td>0.115 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>7.37 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>0.146 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>7.10 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>0.188 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>8.10 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                    </tr>
                  </tbody>
                </table>
              </div>

              <h3 class="title is-4">Reconstruction vs. Similarity</h3>
              <div class="columns">
                <div class="column is-half">
                  <img src="static/images/eval/amd_vs_fvu.png" alt="Feature concordance heatmap for Pythia-160M"
                    loading="lazy" style="width: 100%;" />
                  <p class="has-text-centered"><em>Mean Maximum Concordance (MMCO) showing feature similarity between
                      baseline and Group-SAEs.</em></p>
                </div>
                <div class="column is-half">
                  <img src="static/images/eval/amd_vs_delta_ce.png" alt="Feature distribution across layers"
                    loading="lazy" style="width: 100%;" />
                  <p class="has-text-centered"><em>Feature activation distribution across layers within groups, showing
                      how
                      features spread across similar layers.</em></p>
                </div>
              </div>

              <h3 class="title is-4">Reconstruction vs. Compute (PFLOPs)</h3>
              <div class="columns">
                <div class="column is-half">
                  <img src="static/images/eval/relative_flops_vs_fvu.png"
                    alt="Feature concordance heatmap for Pythia-160M" loading="lazy" style="width: 100%;" />
                  <p class="has-text-centered"><em>Mean Maximum Concordance (MMCO) showing feature similarity between
                      baseline and Group-SAEs.</em></p>
                </div>
                <div class="column is-half">
                  <img src="static/images/eval/relative_flops_vs_delta_ce.png" alt="Feature distribution across layers"
                    loading="lazy" style="width: 100%;" />
                  <p class="has-text-centered"><em>Feature activation distribution across layers within groups, showing
                      how
                      features spread across similar layers.</em></p>
                </div>
              </div>

              <h3 class="title is-4">Auto-Interpretability</h3>
              <div class="columns">
                <div class="column is-half">
                  <img src="static/images/autointerp/detection.png" alt="Feature concordance heatmap for Pythia-160M"
                    loading="lazy" style="width: 100%;" />
                  <p class="has-text-centered"><em>Mean Maximum Concordance (MMCO) showing feature similarity between
                      baseline and Group-SAEs.</em></p>
                </div>
                <div class="column is-half">
                  <img src="static/images/autointerp/fuzzing.png" alt="Feature distribution across layers"
                    loading="lazy" style="width: 100%;" />
                  <p class="has-text-centered"><em>Feature activation distribution across layers within groups, showing
                      how
                      features spread across similar layers.</em></p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End additional analysis -->

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code">
<code>@inproceedings{
  ghilardi2025efficient,
  title={Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups},
  author={Davide Ghilardi and Federico Belotti and Marco Molinari and Tao Ma and Matteo Palmonari},
  booktitle={The 2025 Conference on Empirical Methods in Natural Language Processing},
  year={2025},
  url={https://openreview.net/forum?id=bk4PhF17cm}
}</code>
        </pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>