<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title"
    content="Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta name="keywords"
    content="sparse autoencoders, SAE, interpretability, large language models, LLM, efficiency, layer clustering, Group-SAE">
  <meta name="author" content="Davide Ghilardi, Federico Belotti, Marco Molinari, Tao Ma, Matteo Palmonari">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Group-SAE">
  <meta property="og:title"
    content="Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta property="og:description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta property="og:url" content="https://github.com/ghidav/group-sae-page">
  <meta property="og:image" content="static/images/main.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Group-SAE Research Team">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="sparse autoencoders">
  <meta property="article:tag" content="interpretability">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title"
    content="Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="twitter:description"
    content="Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.">
  <meta name="twitter:image" content="static/images/main.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title"
    content="Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups">
  <meta name="citation_author" content="Davide Ghilardi, Federico Belotti, Marco Molinari, Tao Ma, Matteo Palmonari">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="Research Paper">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups",
    "description": "Group-SAE: A novel approach to train sparse autoencoders efficiently by clustering layers based on angular similarity, reducing training costs by up to 50%.",
    "author": [
      {
        "@type": "Person",
        "name": "Davide Ghilardi",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      },
      {
        "@type": "Person",
        "name": "Federico Belotti",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      },
      {
        "@type": "Person",
        "name": "Marco Molinari",
        "affiliation": {
          "@type": "Organization",
          "name": "London School of Economics"
        }
      },
      {
        "@type": "Person",
        "name": "Tao Ma",
        "affiliation": {
          "@type": "Organization",
          "name": "London School of Economics"
        }
      },
      {
        "@type": "Person",
        "name": "Matteo Palmonari",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Milano Bicocca"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "Research Publication"
    },
    "url": "https://github.com/ghidav/group-sae-page",
    "image": "static/images/main.png",
    "keywords": ["sparse autoencoders", "SAE", "interpretability", "large language models", "efficiency"],
    "abstract": "Sparse Autoencoders (SAEs) have recently been employed as a promising unsupervised approach for understanding the representations of layers of Large Language Models (LLMs). However, with the growth in model size and complexity, training SAEs is computationally intensive. We propose Group-SAE, a novel strategy that groups similar layers and trains a single SAE per group, significantly reducing training costs.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://github.com/ghidav/group-sae-page"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Machine Learning Interpretability"
      },
      {
        "@type": "Thing",
        "name": "Sparse Autoencoders"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Group-SAE Research",
    "url": "https://github.com/your-username/group-sae-page",
    "logo": "static/images/favicon.ico"
  }
  </script>

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      }
    };
  </script>
  <script defer id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!--
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <div class="work-item" style="text-align: center; padding: 20px; color: #666;">
          <p>Related works will be added here soon.</p>
        </div>
      </div>
    </div>
  </div>
  -->

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Group-SAE: Efficient Training of Sparse Autoencoders for Large
                Language Models via Layer Groups</h1>
              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your paper authors and their personal links -->
                <span class="author-block">
                  <a href="https://ghidav.github.io/" target="_blank">Davide Ghilardi</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://belerico.github.io/" target="_blank">Federico Belotti</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.marcomolinari.io/" target="_blank">Marco Molinari</a><sup>*</sup>,
                </span>
                <span class="author-block">
                  <a href="https://xiaobaobaochifan.github.io/" target="_blank">Tao Ma</a>,
                </span>
                <span class="author-block">
                  <a href="https://www.unimib.it/matteo-luigi-palmonari" target="_blank">Matteo Palmonari</a>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your institution and conference/journal info -->
                <span class="author-block">University of Milano Bicocca, LSE<br>EMNLP 2025</span>
                <!-- TODO: Remove this line if no equal contribution -->
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2410.21508" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/ghidav/group-sae" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2410.21508" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/Group-SAE" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="static/images/hf-logo.svg" alt="HF Logo" width="24" height="24">
                      </span>
                      <span>Models</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      </div>
    </section>

    <section class="hero teaser" style="margin-bottom: 3rem;">
      <div class="container is-max-desktop">
        <p class="image">
          <video poster="" id="main" autoplay controls muted loop height="100%">
            <source src="static/videos/main.mp4" type="video/mp4">
          </video>
        </p>
      </div>
    </section>

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Sparse Autoencoders (SAEs) have recently been employed as a promising unsupervised approach for
                understanding the representations of layers of Large Language Models (LLMs). However, with the growth in
                model size and complexity, training SAEs is computationally intensive, as typically one SAE is trained
                for each model layer. To address such limitation, we propose <strong>Group-SAE</strong>, a novel
                strategy to train SAEs. Our method considers the similarity of the residual stream representations
                between contiguous layers to group similar layers and train a single SAE per group. To balance the
                trade-off between efficiency and performance, we further introduce <strong>AMAD</strong> (Average
                Maximum Angular Distance), an empirical metric that guides the selection of an optimal number of groups
                based on representational similarity across layers. Experiments on models from the Pythia family show
                that our approach significantly accelerates training with minimal impact on reconstruction quality and
                comparable downstream task performance and interpretability over baseline SAEs trained layer by layer.
                This method provides an efficient and scalable strategy for training SAEs in modern LLMs.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->


    <!-- Method Section -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h2 class="title">Group-SAE</h2>
              <div class="content">
                <p>
                  Group-SAE is a novel technique that reduces the computational demand for training a suite of SAEs for
                  a given model.
                  We group model layers by the cosine similarity of their activations and train a single SAE per group.
                </p>

                <h3>Grouping strategy</h3>
                <p>
                  Our grouping strategy consists of four steps:
                <ol>
                  <li>We cache 10M activations for each layer of the model from tokens of its training dataset. </li>
                  <li>We compute the average cosine distance for each pair of layer. From this process, we obtain a
                    distance matrix.</li>
                  <li>We apply <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" target="_blank"
                      rel="noopener noreferrer">hierarchical clustering</a> on the distance matrix to obtain the
                    partitions in $g=1,…,G$ groups.</li>
                  <li>The last step consists of computing the AMAD, the average maximum angular distance of groups, for
                    each partition.</li>
                </ol>

                <div class="container is-max-desktop" style="margin-bottom: 3rem; margin-top: 3rem;">
                  <p class="image">
                    <img src="static/gifs/grouping.gif" alt="Illustration of the grouping strategy"
                      style="width: 100%; height: auto;">
                  </p>
                </div>

                <p>
                  Given the AMAD computed at step 4., one can choose the optimal number of groups $\widehat{G}$ as:
                </p>
                <p class="has-text-centered">
                  \[
                  \widehat{G} = \min \{G \in \{1, ..., L-1\} \mid \text{AMAD}(G) < \theta \} \] </p>
                    <p>
                      Intuitively, when $G$ is small, each group aggregates distant layers, increasing AMAD; conversely,
                      when $G=L$ (one group per layer), AMAD becomes zero but no computational savings are achieved.
                      <strong>The goal is therefore to select the smallest $G$ such that the groups remain sufficiently
                        homogeneous</strong>, i.e., AMAD$(G)$ stays below a target threshold.
                    </p>
                    <p>
                      By training Group-SAEs for different models and values of G, and plotting performance metrics
                      against AMAD (see figure below, <i>Figure 2</i> in the paper), we identify an optimal threshold
                      around $\theta^* \approx 0.2$, beyond which reconstruction
                      performance
                      degrades more rapidly.
                    </p>

                    <div class="columns is-centered" style="margin-bottom: 0px;">
                      <div class="column is-half">
                        <div>
                          <img src="static/images/eval/amd_vs_fvu.png" alt="Feature concordance heatmap for Pythia-160M"
                            loading="lazy" style="width: 70%;" />
                        </div>
                      </div>

                      <div class="column is-half">
                        <img src="static/images/eval/amd_vs_delta_ce.png" alt="Feature distribution across layers"
                          loading="lazy" style="width: 70%;" />
                      </div>
                    </div>

                    <div class="column is-full" style="padding: 0px;">
                      <p>
                        <em>$\text{FVU}$ (left) and $\Delta\text{CE}_{\%}$ (right) over $\text{AMAD}(G)$ for every $G \in {1, .
                          . . , L − 1}$. The highlighted star
                          markers represent the baseline SAEs (i.e., with no grouping), while the other points
                          correspond to Group-SAEs,
                          ordered from left to right by increasing AMAD, which reflects a decrease in the number of
                          groups. The shaded area
                          indicates one std.</em>
                      </p>
                    </div>

                    <h3>SAE training</h3>
                    <p>
                      We train both SAEs and Group-SAEs using Top-$K$ activation with $K=128$ and expansion factor of
                      $c=16$ on
                      the residual stream after the MLP contribution of three models of varying sizes from the <a
                        href="https://github.com/EleutherAI/pythia" target="_blank">Pythia
                        family</a> <a class="cite" data-cite="biderman2023pythia"></a>: Pythia 160M, Pythia 410M, and
                      Pythia
                      1B.<br><br>

                      The reconstruction objective is the Fraction of Variance Unexplained (FVU) <a class="cite"
                        data-cite="lawson2024mlsae"></a>, i.e.:
                      $$
                      \text{FVU}(\mathbf X, \widehat{\mathbf
                      X})=\frac{\lVert\mathbf{X}-\widehat{\mathbf{X}}\rVert_F^2}{\lVert \mathbf{X} -
                      \overline{\mathbf{X}} \rVert_F^2}
                      $$
                      where $\mathbf X, \widehat{\mathbf X}, \overline{\mathbf{X}} \in \mathbb R^{B \times n}$ represent a batch of activations
                      SAE reconstructions, and average batch activations, respectively.<br>
                      The total loss is then:
                      $$
                      \mathcal L(\mathbf X)=\text{FVU}(\mathbf X,\widehat{\mathbf
                      X})+\alpha_{\text{aux}}\cdot\text{AuxK}(\mathbf X,\widehat{\mathbf X})
                      $$
                      with $\alpha_{\text{aux}}=\tfrac{1}{32}$ and $\text{AuxK}(\mathbf X,\widehat{\mathbf X})$ is the
                      auxiliary loos function defined in <a class="cite" data-cite="gao2024topk"></a>.<br><br>

                      <!--We use Adam ($\beta_1{=}0.9,\beta_2{=}0.999$), learning rate
                      $\text{lr}=2{\times}10^{-4}/\sqrt{c \cdot n/2^{14}}$. We use a
                      batch size of $131072, 65536\text{ and } 32768$ for the three models, respectively, to maximize
                      computational usage. Following <a class="cite" data-cite="bricken2023monosemanticity"></a> we
                      constrain the decoder columns
                      to have unit norm. Additionally, we normalize the activations to
                      have mean squared $\ell_2$ norm of 1 during SAE training, by first estimating the norm scaling
                      factor over 5 million
                      tokens of our train set. We mark a latent as “dead” after 10M
                      tokens.<br><br> -->

                      To train all the SAEs, we sample 1 billion tokens from the <a
                        href="https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated" target="_blank">Pile
                        dataset</a> <a class="cite" data-cite="pile"></a>.
                      For Group-SAEs, <strong>activations in each batch are randomly drawn from the layers within the
                        target group</strong>, so that every SAE (baseline and group) processes exactly 1B tokens.
                    </p>

                    <div class="container is-max-desktop" style="margin-bottom: 3rem; margin-top: 3rem;">
                      <p class="image">
                        <img src="static/gifs/training.gif" alt="Illustration of the training strategy"
                          style="width: 90%; height: auto;">
                      </p>
                    </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End method section -->


    <!-- Evaluations -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-four-fifths">

              <h3 class="title is-4">Comparison with baselines</h3>
              <div class="content">
                <p class="has-text-justified">
                  We compare our method against two baselines:
                <ul>
                  <li><strong>Evenly spaced SAEs</strong>: instead of grouping with AMAD, we train Group-SAE on
                    evenly-spaced groups.</li>
                  <li><strong>Smaller SAEs</strong>: for all layers, we train standard SAEs with a lower expansion
                    factor.</li>
                </ul>

                $\text{FVU}$ and $\Delta\text{CE}$ for different approaches across model sizes. Our AMAD-based grouping
                strategy
                attains lower $\text{FVU}$ and $\Delta\text{CE}$ than the baselines: evenly spaced groups and smaller
                SAEs
                trained on all layers. Both baselines use the same training FLOPs as our method. Percentages denote
                improvement over the <em>Smaller SAEs (All layers)</em> baseline (positive&nbsp;=&nbsp;better).
                </p>
              </div>
              <div class="table-container">
                <table class="table is-striped is-hoverable is-fullwidth evaluation-table">
                  <thead>
                    <tr>
                      <th rowspan="2">Approach</th>
                      <th colspan="2" class="has-text-centered">Pythia-160M</th>
                      <th colspan="2" class="has-text-centered">Pythia-410M</th>
                      <th colspan="2" class="has-text-centered">Pythia-1B</th>
                    </tr>
                    <tr>
                      <th>FVU</th>
                      <th>$\Delta\text{CE}_{\%}$</th>
                      <th>FVU</th>
                      <th>$\Delta\text{CE}_{\%}$</th>
                      <th>FVU</th>
                      <th>$\Delta\text{CE}_{\%}$</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <th scope="row">Group SAEs (AMAD with $\widehat{G}$ groups)</th>
                      <td><strong>0.108</strong> <span class="is-size-7 has-text-grey">(+6.1%)</span></td>
                      <td>6.01 <span class="is-size-7 has-text-grey">(+18.5%)</span></td>
                      <td><strong>0.138</strong> <span class="is-size-7 has-text-grey">(+5.5%)</span></td>
                      <td><strong>5.94</strong> <span class="is-size-7 has-text-grey">(+16.3%)</span></td>
                      <td><strong>0.182</strong> <span class="is-size-7 has-text-grey">(+3.2%)</span></td>
                      <td><strong>6.43</strong> <span class="is-size-7 has-text-grey">(+20.6%)</span></td>
                    </tr>
                    <tr>
                      <th scope="row">Group SAEs (Evenly spaced with $\widehat{G}$ groups)</th>
                      <td>0.114 <span class="is-size-7 has-text-grey">(+0.9%)</span></td>
                      <td><strong>5.40</strong> <span class="is-size-7 has-text-grey">(+26.7%)</span></td>
                      <td>0.145 <span class="is-size-7 has-text-grey">(+0.7%)</span></td>
                      <td>6.01 <span class="is-size-7 has-text-grey">(+15.4%)</span></td>
                      <td>0.189 <span class="is-size-7 has-text-grey">(-0.5%)</span></td>
                      <td>6.63 <span class="is-size-7 has-text-grey">(+18.1%)</span></td>
                    </tr>
                    <tr>
                      <th scope="row">Smaller SAEs (All layers, same PFLOPs)</th>
                      <td>0.115 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>7.37 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>0.146 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>7.10 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>0.188 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                      <td>8.10 <span class="is-size-7 has-text-grey">(+0.0%)</span></td>
                    </tr>
                  </tbody>
                </table>
              </div>

              <br>

              <h3 class="title is-4">Efficiency Improvements</h3>

              <div class="content">
                <p>
                  The computational cost, in FLOPs, of training a SAE can be divided into two main components:
                </p>

                <ul>
                  <li><strong>Activation caching</strong> (A): FLOPs required to generate the model's activations, which
                    are used for training the SAE.</li>
                  <li><strong>SAE Training</strong> (T): FLOPs involved in optimizing a single SAE using the cached
                    activations.</li>
                </ul>

                <p>
                  Thus, the total cost of training SAEs across all residual stream layers of a model is given by $A +
                  L\,T$.
                  Since both baseline and Group-SAE share the same architecture and undergo the same training process
                  for a single SAE,
                  the total cost of training all Group-SAEs is $A + G\,T$.
                </p>

                <p>The resulting compute savings, $\Delta(G)$, quantifying the relative change in total FLOPs when
                  applying Group-SAEs instead of per-layer SAEs, is defined as:</p>

                <p>\[\Delta(G) = 1 - \frac{A + G\,T}{A + L\,T}.\]</p>

                <p>Since our method does not alter either $A$ or $T$, the efficiency gains of Group-SAEs are primarily
                  determined by the $G/L$ ratio. The table below shows performance improvements of Group-SAE for each
                  model when training a full suite of SAEs.</p>
              </div>

              <div class="container is-max-desktop" style="margin-bottom: 3rem; margin-top: 3rem;">
                <table class="table is-striped is-hoverable is-fullwidth evaluation-table"
                  style="width: 90%; height: auto; margin: auto;">
                  <thead>
                    <tr>
                      <th style="text-align: left;">Model</th>
                      <th>$\widehat G$</th>
                      <th>$A+L\,T$</th>
                      <th>$A+\widehat G\,T$</th>
                      <th>$\Delta_{\%} (\widehat G)$</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td style="text-align: left;">Pythia 160M</td>
                      <td>6</td>
                      <td>1.34</td>
                      <td><b>0.77</b></td>
                      <td>+42.5%</td>
                    </tr>
                    <tr>
                      <td style="text-align: left;">Pythia 410M</td>
                      <td>9</td>
                      <td>4.73</td>
                      <td><b>2.21</b></td>
                      <td>+53.3%</td>
                    </tr>
                    <tr>
                      <td style="text-align: left;">Pythia 1B</td>
                      <td>6</td>
                      <td>12.48</td>
                      <td><b>5.77</b></td>
                      <td>+53.7%</td>
                    </tr>
                  </tbody>
                </table>
                <p class="has-text-centered" style="margin-top: 1rem;">
                  <em>Comparison of FLOPs (10<sup>18</sup>) required for caching activations and training Baseline and
                    Group SAEs on 1B tokens,
                    covering all layers with an expansion factor of 16 and
                    Ĝ = min { G | AMAD(G) &lt; 0.2 }.</em>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End additional analysis -->

    <section class="section" id="references">
      <div class="container is-max-desktop content">
        <h2 class="title">References</h2>
        <div id="refs"></div>
      </div>
    </section>


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code">
<code>@inproceedings{
  ghilardi2025efficient,
  title={Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups},
  author={Davide Ghilardi and Federico Belotti and Marco Molinari and Tao Ma and Matteo Palmonari},
  booktitle={The 2025 Conference on Empirical Methods in Natural Language Processing},
  year={2025},
  url={https://openreview.net/forum?id=bk4PhF17cm}
}</code>
        </pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

    <script src="https://cdn.jsdelivr.net/npm/citation-js@0.4.0-8" type="text/javascript"></script>
    <script>
      (async () => {
        const el = document.getElementById('refs');
        if (!el) return;

        const Cite =
          (typeof window.require === 'function' && window.require('citation-js')) ||
          window.Cite || window.Citation;

        if (!Cite) {
          el.textContent = 'Citation.js loaded but API not found.';
          return;
        }

        // Load refs.bib, fallback to <pre><code>…</code></pre> in #BibTeX
        let bib;
        try {
          const r = await fetch('static/bib/refs.bib', { cache: 'no-cache' });
          if (!r.ok) throw new Error('refs.bib not found');
          bib = await r.text();
        } catch {
          const pre = document.querySelector('#BibTeX pre code');
          if (!pre) {
            el.textContent = 'References failed to load.';
            return;
          }
          bib = pre.textContent;
        }

        let cite;
        try {
          cite = new Cite(bib);
        } catch (e) {
          console.error('[citations] parse error:', e);
          el.textContent = 'References failed to parse.';
          return;
        }

        const entries = cite.data || [];
        const html = cite.format('bibliography', {
          format: 'html',
          template: 'ama',   // or 'apa', 'vancouver', ...
          lang: 'en-US'
        });
        el.innerHTML = html;

        (function makeNumbered() {
          const root = document.getElementById('refs');
          const body = root.querySelector('.csl-bib-body');
          if (!body) return;

          // Collect entries
          const entries = [...body.querySelectorAll('.csl-entry')];
          if (!entries.length) return;

          // Build an ordered list
          const ol = document.createElement('ol');
          // preserve class so any CSL CSS you add still applies
          ol.className = body.className || 'csl-bib-body';

          entries.forEach((div, i) => {
            const li = document.createElement('li');
            li.className = div.className || 'csl-entry';
            li.innerHTML = div.innerHTML;
            ol.appendChild(li);
          });

          // Replace the CSL body with the ordered list
          body.replaceWith(ol);
        })();

        // Optional: stable IDs for inline links <a href="#ref-<key>">
        const list = el.querySelector('ol, ul') || el;
        [...list.children].forEach((li, i) => {
          const key = entries[i] && entries[i].id;
          if (key) li.id = `ref-${key}`;
        });
      })();
    </script>
    <script>
      // Build a map: bibkey -> citation number
      function buildCiteIndex() {
        const refsEl = document.getElementById('refs');
        const list = refsEl && refsEl.querySelector('ol');
        const index = new Map();
        if (!list) return index; // not ready yet
        [...list.children].forEach((li, i) => {
          const m = /^ref-(.+)$/.exec(li.id || '');
          if (m) index.set(m[1], i + 1);
        });
        return index;
      }

      function formatBracketed(nums) {
        nums.sort((a, b) => a - b);
        const ranges = [];
        for (let i = 0; i < nums.length;) {
          let j = i;
          while (j + 1 < nums.length && nums[j + 1] === nums[j] + 1) j++;
          ranges.push(i === j ? `${nums[i]}` : `${nums[i]}–${nums[j]}`);
          i = j + 1;
        }
        return `[${ranges.join(', ')}]`;
      }

      function renderInlineCitations() {
        const idx = buildCiteIndex();
        if (idx.size === 0) return false; // not ready yet
        document.querySelectorAll('a.cite[data-cite]').forEach(a => {
          const keys = a.getAttribute('data-cite').split(',').map(s => s.trim()).filter(Boolean);
          const nums = keys.map(k => idx.get(k)).filter(n => Number.isInteger(n));
          if (!nums.length) return;
          a.textContent = formatBracketed(nums);
          a.setAttribute('href', `#ref-${keys[0]}`);
        });
        return true;
      }

      // Retry until #refs has an <ol>
      function renderInlineCitationsWhenReady(attempt = 0) {
        if (renderInlineCitations()) return;
        if (attempt > 200) return; // ~10s cap at 50ms intervals
        setTimeout(() => renderInlineCitationsWhenReady(attempt + 1), 50);
      }

      // Call this AFTER you kick off Citation.js rendering
      renderInlineCitationsWhenReady();
    </script>


</body>

</html>